{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zH6Vc-v21GbH",
        "D-g3oW1x0fl2",
        "OJ9e_0k21Mfl",
        "4M4Ratgk1PJm",
        "oqaCixuLPWOJ",
        "JMyvB8mAPcUU",
        "WADNyp43P5LQ",
        "28cE-2mDQimr",
        "mTfZvaLDQimr",
        "GDIUnQ6RQims",
        "L5uJIjwQSPmT",
        "4b1Wk0O3SPmT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svenr4xYNIy_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
        "from imblearn.under_sampling import TomekLinks, OneSidedSelection,EditedNearestNeighbours\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score"
      ],
      "metadata": {
        "id": "jnF46btkNm-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data (Category)"
      ],
      "metadata": {
        "id": "zH6Vc-v21GbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/Datasets/VirusShare_Opcodes/csvFiles/DATASET_SHUFFLED_VirusShare_proportions_and_targets.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/VirusShare_Opcodes/csvFiles/MalwareCategory_DATASET_Increased_Imbalance.csv')\n",
        "\n",
        "# Delete rows where 'Category' is equal to 'Unknown'\n",
        "df = df[(df['Category'] != 'Unknown')]\n",
        "df['Category'].value_counts()"
      ],
      "metadata": {
        "id": "897K7LesNsXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data (Family)"
      ],
      "metadata": {
        "id": "D-g3oW1x0fl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/Datasets/VirusShare_Opcodes/csvFiles/DATASET_SHUFFLED_VirusShare_proportions_and_targets.csv')\\\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/VirusShare_Opcodes/csvFiles/MalwareFamily_DATASET_FINAL_Increased_Imbalance.csv')\n",
        "# Delete rows where 'name' is equal to 'Unknown_Family'\n",
        "df = df[(df['name'] != 'Unknown_Family')]\n",
        "df['name'].value_counts()"
      ],
      "metadata": {
        "id": "8_CNyFAVWwGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features/Targets"
      ],
      "metadata": {
        "id": "OJ9e_0k21Mfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the features\n",
        "features = df.drop(columns=[\"file_name\",\"name\", \"Category\",\"Category Target\", \"Family Target\"]).astype(float)\n",
        "\n",
        "# Prepare the target\n",
        "targets = df[\"Category Target\"].astype(int)\n",
        "\n",
        "# # Prepare the target\n",
        "# targets = df[\"Family Target\"].astype(int)"
      ],
      "metadata": {
        "id": "3l_P0TE-OvmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Test Split"
      ],
      "metadata": {
        "id": "4M4Ratgk1PJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features,\n",
        "    targets,\n",
        "    stratify=targets,\n",
        "    test_size=0.2,\n",
        "    random_state=0)\n",
        "\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "gbChHFUIPL50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GridSearchCV"
      ],
      "metadata": {
        "id": "z4jIZ5mrXPLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classifiers\n",
        "classifiers = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=0),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(200,200, 200),early_stopping=True,random_state=0, n_iter_no_change= 5),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'SVM': SVC(kernel='rbf', random_state=0,class_weight= None, gamma= 'scale')\n",
        "}"
      ],
      "metadata": {
        "id": "fo828YOgb6QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE + TL"
      ],
      "metadata": {
        "id": "oqaCixuLPWOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "smote = SMOTE(random_state=0)\n",
        "tomek_links = TomekLinks(random_state=0)\n",
        "\n",
        "# sm_8 = SMOTE(random_state=0,sampling_strategy='auto', k_neighbors=8)\n",
        "# X_resampled, y_resampled = sm_8.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of SMOTE and Tomek Links\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with SMOTE and Tomek Links\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(smote, tomek_links, clf)\n",
        "\n",
        "    # Set the parameter grid for SMOTE k_neighbors and Tomek Links sampling_strategy\n",
        "    param_grid = {\n",
        "        'smote__k_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for simplicity\n",
        "        'tomeklinks__sampling_strategy': ['auto', 'majority']\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "XUvj-SqvNTJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE + ENN"
      ],
      "metadata": {
        "id": "JMyvB8mAPcUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "smote = SMOTE(random_state=0)\n",
        "enn = EditedNearestNeighbours(random_state=0)\n",
        "\n",
        "# sm_8 = SMOTE(random_state=0,sampling_strategy='auto', k_neighbors=8)\n",
        "# X_resampled, y_resampled = sm_8.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of SMOTE and ENN\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with SMOTE and ENN\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(smote, enn, clf)\n",
        "\n",
        "    # Set the parameter grid for SMOTE and ENN\n",
        "    param_grid = {\n",
        "        'smote__k_neighbors': list(range(1, 21)),  # Testing a range from 1 to 2 for SMOTE\n",
        "        'editednearestneighbours__n_neighbors': list(range(1, 11)),  # Testing a range from 1 to 10 for ENN\n",
        "        'editednearestneighbours__sampling_strategy': ['auto', 'majority']\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "d7yaJtn5Ne6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE + OSS"
      ],
      "metadata": {
        "id": "WADNyp43P5LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "smote = SMOTE(random_state=0)\n",
        "oss = OneSidedSelection(random_state=0)\n",
        "\n",
        "# sm_8 = SMOTE(random_state=0,sampling_strategy='auto', k_neighbors=8)\n",
        "# X_resampled, y_resampled = sm_8.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of SMOTE and OSS\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with SMOTE and OSS\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(smote, oss, clf)\n",
        "\n",
        "    # Set the parameter grid for SMOTE and OSS\n",
        "    param_grid = {\n",
        "        'smote__k_neighbors': list(range(1, 20)),  # Testing a range from 1 to 20 for SMOTE\n",
        "        'onesidedselection__n_neighbors': list(range(1, 11)),  # Testing a range from 1 to 10 for OSS\n",
        "        'onesidedselection__sampling_strategy': ['auto', 'majority']\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "6pT_XaQ9P4oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADASYN + TL"
      ],
      "metadata": {
        "id": "28cE-2mDQimr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "adasyn = ADASYN(random_state=0)\n",
        "tomek_links = TomekLinks(random_state=0)\n",
        "\n",
        "# ada_5 = ADASYN(sampling_strategy='auto', random_state=0,n_neighbors=5)\n",
        "# X_resampled, y_resampled = ada_5.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of ADASYN and Tomek Links\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with ADASYN and Tomek Links\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(adasyn, tomek_links, clf)\n",
        "\n",
        "    # Set the parameter grid for ADASYN and Tomek Links\n",
        "    param_grid = {\n",
        "        'adasyn__n_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for ADASYN\n",
        "        'tomeklinks__sampling_strategy': ['auto', 'majority']  # Sampling strategies for Tomek Links\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "hQ2ULjCqQimr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADASYN + ENN"
      ],
      "metadata": {
        "id": "mTfZvaLDQimr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "adasyn = ADASYN(random_state=0)\n",
        "enn = EditedNearestNeighbours( random_state=0)\n",
        "\n",
        "# ada_5 = ADASYN(sampling_strategy='auto', random_state=0,n_neighbors=5)\n",
        "# X_resampled, y_resampled = ada_5.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of ADASYN and ENN\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with ADASYN and ENN\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(adasyn, enn, clf)\n",
        "\n",
        "    # Set the parameter grid for ADASYN and ENN\n",
        "    param_grid = {\n",
        "        'adasyn__n_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for ADASYN\n",
        "        'editednearestneighbours__n_neighbors': list(range(1, 11)),  # Testing a range from 1 to 10 for ENN\n",
        "        'editednearestneighbours__sampling_strategy': ['auto', 'majority']  # Sampling strategies for ENN\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "w44KOPISQims"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADASYN + OSS"
      ],
      "metadata": {
        "id": "GDIUnQ6RQims"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "adasyn = ADASYN(random_state=0)\n",
        "oss = OneSidedSelection(random_state=0)\n",
        "\n",
        "# ada_5 = ADASYN(sampling_strategy='auto', random_state=0,n_neighbors=5)\n",
        "# X_resampled, y_resampled = ada_5.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of ADASYN and OSS\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with ADASYN and OSS\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(adasyn, enn, clf)\n",
        "\n",
        "    # Set the parameter grid for ADASYN and OSS\n",
        "    param_grid = {\n",
        "        'adasyn__n_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for ADASYN\n",
        "        'onesidedselection__n_neighbors': list(range(1, 11)),  # Testing a range from 1 to 10 for OSS\n",
        "        'onesidedselection__sampling_strategy': ['auto', 'majority']\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "4ytP7SelQims"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BSMOTE + TL"
      ],
      "metadata": {
        "id": "L5uJIjwQSPmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "borderline_smote = BorderlineSMOTE(random_state=0)\n",
        "tomek_links = TomekLinks(random_state=0)\n",
        "\n",
        "# bsm_3_18 = BorderlineSMOTE(sampling_strategy='auto', random_state=0,k_neighbors=3, m_neighbors=18)\n",
        "# X_resampled, y_resampled = bsm_3_18.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of BorderlineSMOTE and Tomek Links\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with BorderlineSMOTE and Tomek Links\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(borderline_smote, tomek_links, clf)\n",
        "\n",
        "    # Set the parameter grid for BorderlineSMOTE and Tomek Links\n",
        "    param_grid = {\n",
        "        'borderlinesmote__k_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for BorderlineSMOTE k_neighbors\n",
        "        'borderlinesmote__m_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for BorderlineSMOTE m_neighbors\n",
        "        'tomeklinks__sampling_strategy': ['auto', 'majority']  # Sampling strategies for Tomek Links\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "q9eqCkyuSPmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BSMOTE + ENN"
      ],
      "metadata": {
        "id": "4b1Wk0O3SPmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "borderline_smote = BorderlineSMOTE(random_state=0)\n",
        "enn = EditedNearestNeighbours(random_state=0)\n",
        "\n",
        "# bsm_3_18 = BorderlineSMOTE(sampling_strategy='auto', random_state=0,k_neighbors=3, m_neighbors=18)\n",
        "# X_resampled, y_resampled = bsm_3_18.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of BorderlineSMOTE and ENN\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with BorderlineSMOTE and ENN\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(borderline_smote, enn, clf)\n",
        "\n",
        "    # Set the parameter grid for BorderlineSMOTE and ENN\n",
        "    param_grid = {\n",
        "        'borderlinesmote__k_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for k_neighbors in BorderlineSMOTE\n",
        "        'borderlinesmote__m_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for m_neighbors in BorderlineSMOTE\n",
        "        'editednearestneighbours__n_neighbors': list(range(1, 11)),  # Testing a range from 1 to 10 for n_neighbors in ENN\n",
        "        'editednearestneighbours__sampling_strategy': ['auto', 'majority']  # Sampling strategies for ENN\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "boq29CpsSPmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BSMOTE + OSS"
      ],
      "metadata": {
        "id": "N0XZ0KRiSPmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling techniques\n",
        "borderline_smote = BorderlineSMOTE(random_state=0)\n",
        "oss = OneSidedSelection(random_state=0)\n",
        "\n",
        "# bsm_3_18 = BorderlineSMOTE(sampling_strategy='auto', random_state=0,k_neighbors=3, m_neighbors=18)\n",
        "# X_resampled, y_resampled = bsm_3_18.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a custom scorer that uses the macro average\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# List to hold all grid search results\n",
        "results = []\n",
        "\n",
        "# Test each classifier with the combination of BorderlineSMOTE and OSS\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Testing {name} with BorderlineSMOTE and OSS\")\n",
        "\n",
        "    # Define the pipeline with make_pipeline from imblearn\n",
        "    pipeline = make_pipeline(borderline_smote, oss, clf)\n",
        "\n",
        "    # Set the parameter grid for BorderlineSMOTE and OSS\n",
        "    param_grid = {\n",
        "        'borderlinesmote__k_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for k_neighbors in BorderlineSMOTE\n",
        "        'borderlinesmote__m_neighbors': list(range(1, 21)),  # Testing a range from 1 to 20 for m_neighbors in BorderlineSMOTE\n",
        "        'onesidedselection__n_neighbors': list(range(1, 11)),  # Testing a range from 1 to 10 for n_neighbors in OSS\n",
        "        'onesidedselection__sampling_strategy': ['auto', 'majority']\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1_macro_scorer, verbose=1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    # grid_search.fit(X_resampled, y_resampled)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator, its parameters, and the corresponding score\n",
        "    results.append({\n",
        "        'Classifier': name,\n",
        "        'Best_Params': grid_search.best_params_,\n",
        "        'Best_Score': grid_search.best_score_\n",
        "    })\n",
        "\n",
        "# Output the results\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}, Best Score: {result['Best_Score']}, Params: {result['Best_Params']}\")\n"
      ],
      "metadata": {
        "id": "gVT6KePWSPmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}